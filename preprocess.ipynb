{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from prepare import prep_create_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('indeed-data-jobs-FINAL.json')\n",
    "df = prep_create_labels(df).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize description text**: normalized text by lowercasing all letters, removes any inconsistencies in unicode character encoding, convert the resulting string to the ASCII character set. We'll ignore any errors in conversion, meaning we'll drop anything that isn't an ASCII character. Lastly,  turn the resulting bytes object back into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.job_description[0]\n",
    "\n",
    "string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "string = re.sub(r'[^\\w\\s]', '', string).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize description text:** break words and any punctuation left over into discrete units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.job_description[0]\n",
    "\n",
    "# Create tokenizer.\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "# Use tokenizer\n",
    "string = tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming text**: use the base form of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.job_description[0]\n",
    "\n",
    "# Create porter stemmer.\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "# Use the stemmer to stem each word in the list of words we created by using split.\n",
    "stems = [ps.stem(word) for word in string.split()]\n",
    "\n",
    "# Join our lists of words into a string again and assign to a variable.\n",
    "string = ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatizing text**: he base form in this case is known as the root word, but not the root stem. The difference is that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.job_description[0]\n",
    "\n",
    "# Create the lemmatizer.\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Use the lemmatizer on each word in the list of words we created by using split.\n",
    "lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "\n",
    "# Join our list of words into a string again and assign to a variable.\n",
    "string = ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove stopwords from text**: Words which have little or no significance, especially when constructing meaningful features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.job_description[0]\n",
    "extra_words=[]\n",
    "exclude_words=[]\n",
    "\n",
    "# Create stopword_list.\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "# Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "stopword_list = set(stopword_list) - set(exclude_words)\n",
    "\n",
    "# Add in 'extra_words' to stopword_list.\n",
    "stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "# Split words in string.\n",
    "words = string.split()\n",
    "\n",
    "# Create a list of words from my string with stopwords removed and assign to variable.\n",
    "filtered_words = [word for word in words if word not in stopword_list]\n",
    "\n",
    "# Join words in the list back into strings and assign to a variable.\n",
    "string_without_stopwords = ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "\n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_job_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    # drops duplicates but keeps the first instance\n",
    "    df = df.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(lemmatize)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean).apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean).apply(lemmatize)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import prep_job_data, split_job_data, add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_job_data(df, 'job_description', extra_words=['job', 'description']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the length of the job description (word count), and a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that is a list of each word for each repo \n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.clean] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column name will be words, and the column will contain lists of the words in each doc\n",
    "df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.words.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that shows the length \n",
    "doc_length = [len(wordlist) for wordlist in df.words]\n",
    "df = pd.concat([df, pd.DataFrame({'doc_length': doc_length})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### FINALIZE: Let's test the functions & split the data into train, validate, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>is_remote</th>\n",
       "      <th>salary</th>\n",
       "      <th>post_date</th>\n",
       "      <th>date_accessed</th>\n",
       "      <th>job_description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ForMotiv</td>\n",
       "      <td>Remote</td>\n",
       "      <td>1</td>\n",
       "      <td>$75,000 - $120,000 a year</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Has it ever occurred to you that as the Intern...</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Redzara.com</td>\n",
       "      <td>Remote</td>\n",
       "      <td>1</td>\n",
       "      <td>$35 - $80 an hour</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Only GC / EAD only. No C2CBackground screening...</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Nova Collective</td>\n",
       "      <td>Remote</td>\n",
       "      <td>1</td>\n",
       "      <td>$35 - $48 an hour</td>\n",
       "      <td>24 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Are you a data scientist who is really excited...</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early Career Data Scientist - Applied Math</td>\n",
       "      <td>Pacific Northwest National Laboratory</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Organization and Job ID Job ID: 311747 Directo...</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVP, Data Scientist</td>\n",
       "      <td>Synchrony</td>\n",
       "      <td>Alpharetta, GA 30005</td>\n",
       "      <td>1</td>\n",
       "      <td>$60,000 - $130,000 a year</td>\n",
       "      <td>7 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Job Description: Role Summary/Purpose: This ex...</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    job_title  \\\n",
       "0                              Data Scientist   \n",
       "1                              Data Scientist   \n",
       "2                              Data Scientist   \n",
       "3  Early Career Data Scientist - Applied Math   \n",
       "4                         AVP, Data Scientist   \n",
       "\n",
       "                                 company              location  is_remote  \\\n",
       "0                               ForMotiv                Remote          1   \n",
       "1                            Redzara.com                Remote          1   \n",
       "2                        Nova Collective                Remote          1   \n",
       "3  Pacific Northwest National Laboratory           Seattle, WA          0   \n",
       "4                              Synchrony  Alpharetta, GA 30005          1   \n",
       "\n",
       "                      salary     post_date date_accessed  \\\n",
       "0  $75,000 - $120,000 a year  30+ days ago    2021-03-05   \n",
       "1          $35 - $80 an hour   10 days ago    2021-03-05   \n",
       "2          $35 - $48 an hour   24 days ago    2021-03-05   \n",
       "3                                1 day ago    2021-03-05   \n",
       "4  $60,000 - $130,000 a year    7 days ago    2021-03-05   \n",
       "\n",
       "                                     job_description label  \n",
       "0  Has it ever occurred to you that as the Intern...    DS  \n",
       "1  Only GC / EAD only. No C2CBackground screening...    DS  \n",
       "2  Are you a data scientist who is really excited...    DS  \n",
       "3  Organization and Job ID Job ID: 311747 Directo...    DS  \n",
       "4  Job Description: Role Summary/Purpose: This ex...    DS  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('indeed-data-jobs-FINAL.json')\n",
    "df = prep_create_labels(df).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import prep_job_data, split_job_data, add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandmarz/data-science-projects/nlp-data-job-classifier/preprocess.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  .apply(lemmatize)\n",
      "/Users/brandmarz/data-science-projects/nlp-data-job-classifier/preprocess.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['stemmed'] = df[column].apply(basic_clean).apply(stem)\n",
      "/Users/brandmarz/data-science-projects/nlp-data-job-classifier/preprocess.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lemmatized'] = df[column].apply(basic_clean).apply(lemmatize)\n"
     ]
    }
   ],
   "source": [
    "df = prep_job_data(df, 'job_description', extra_words=['job', 'description']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>is_remote</th>\n",
       "      <th>salary</th>\n",
       "      <th>post_date</th>\n",
       "      <th>date_accessed</th>\n",
       "      <th>job_description</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>words</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>Data Engineer - Data Warehouse - Entry Level</td>\n",
       "      <td>Pearson</td>\n",
       "      <td>Durham, NC</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Description We are the world’s learning compan...</td>\n",
       "      <td>DE</td>\n",
       "      <td>world learning company 24000 employee operatin...</td>\n",
       "      <td>descript we are the world learn compani with m...</td>\n",
       "      <td>description we are the world learning company ...</td>\n",
       "      <td>[world, learning, company, 24000, employee, op...</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bind</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Bind was formed in 2016 by veteran health insu...</td>\n",
       "      <td>DS</td>\n",
       "      <td>bind formed 2016 veteran health insurance inno...</td>\n",
       "      <td>bind wa form in 2016 by veteran health insur i...</td>\n",
       "      <td>bind wa formed in 2016 by veteran health insur...</td>\n",
       "      <td>[bind, formed, 2016, veteran, health, insuranc...</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>AI/ML - Machine Learning Engineer, Siri Unders...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>27 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Summary Posted: Feb 12, 2021 Weekly Hours: 40 ...</td>\n",
       "      <td>MLE</td>\n",
       "      <td>summary posted feb 12 2021 weekly hour 40 role...</td>\n",
       "      <td>summari post feb 12 2021 weekli hour 40 role n...</td>\n",
       "      <td>summary posted feb 12 2021 weekly hour 40 role...</td>\n",
       "      <td>[summary, posted, feb, 12, 2021, weekly, hour,...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Machine Learning Engineer - Office of the CTO ...</td>\n",
       "      <td>VMware</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Machine Learning Engineer - Office of the CTO ...</td>\n",
       "      <td>MLE</td>\n",
       "      <td>machine learning engineer office cto xlabs vmw...</td>\n",
       "      <td>machin learn engin offic of the cto xlab vmwar...</td>\n",
       "      <td>machine learning engineer office of the cto xl...</td>\n",
       "      <td>[machine, learning, engineer, office, cto, xla...</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>SHGT</td>\n",
       "      <td>Remote</td>\n",
       "      <td>1</td>\n",
       "      <td>$87,244 - $199,682 a year</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>Strong in Python scripting, minimum 4+ yrs,Mus...</td>\n",
       "      <td>DE</td>\n",
       "      <td>strong python scripting minimum 4 yrsmust hand...</td>\n",
       "      <td>strong in python script minimum 4 yrsmust have...</td>\n",
       "      <td>strong in python scripting minimum 4 yrsmust h...</td>\n",
       "      <td>[strong, python, scripting, minimumyrsmust, ha...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             job_title  company  \\\n",
       "386       Data Engineer - Data Warehouse - Entry Level  Pearson   \n",
       "124                                     Data Scientist     Bind   \n",
       "580  AI/ML - Machine Learning Engineer, Siri Unders...    Apple   \n",
       "583  Machine Learning Engineer - Office of the CTO ...   VMware   \n",
       "362                                      Data Engineer     SHGT   \n",
       "\n",
       "          location  is_remote                     salary     post_date  \\\n",
       "386     Durham, NC          1                             30+ days ago   \n",
       "124      Minnesota          1                               2 days ago   \n",
       "580  Cambridge, MA          0                              27 days ago   \n",
       "583     Austin, TX          1                              10 days ago   \n",
       "362         Remote          1  $87,244 - $199,682 a year    4 days ago   \n",
       "\n",
       "    date_accessed                                    job_description label  \\\n",
       "386    2021-03-05  Description We are the world’s learning compan...    DE   \n",
       "124    2021-03-05  Bind was formed in 2016 by veteran health insu...    DS   \n",
       "580    2021-03-05  Summary Posted: Feb 12, 2021 Weekly Hours: 40 ...   MLE   \n",
       "583    2021-03-05  Machine Learning Engineer - Office of the CTO ...   MLE   \n",
       "362    2021-03-05  Strong in Python scripting, minimum 4+ yrs,Mus...    DE   \n",
       "\n",
       "                                                 clean  \\\n",
       "386  world learning company 24000 employee operatin...   \n",
       "124  bind formed 2016 veteran health insurance inno...   \n",
       "580  summary posted feb 12 2021 weekly hour 40 role...   \n",
       "583  machine learning engineer office cto xlabs vmw...   \n",
       "362  strong python scripting minimum 4 yrsmust hand...   \n",
       "\n",
       "                                               stemmed  \\\n",
       "386  descript we are the world learn compani with m...   \n",
       "124  bind wa form in 2016 by veteran health insur i...   \n",
       "580  summari post feb 12 2021 weekli hour 40 role n...   \n",
       "583  machin learn engin offic of the cto xlab vmwar...   \n",
       "362  strong in python script minimum 4 yrsmust have...   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "386  description we are the world learning company ...   \n",
       "124  bind wa formed in 2016 by veteran health insur...   \n",
       "580  summary posted feb 12 2021 weekly hour 40 role...   \n",
       "583  machine learning engineer office of the cto xl...   \n",
       "362  strong in python scripting minimum 4 yrsmust h...   \n",
       "\n",
       "                                                 words  doc_length  \n",
       "386  [world, learning, company, 24000, employee, op...         303  \n",
       "124  [bind, formed, 2016, veteran, health, insuranc...         461  \n",
       "580  [summary, posted, feb, 12, 2021, weekly, hour,...         136  \n",
       "583  [machine, learning, engineer, office, cto, xla...         406  \n",
       "362  [strong, python, scripting, minimumyrsmust, ha...         186  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = add_columns(df)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.6\n",
      "validate: 0.2\n",
      "test: 0.2\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = split_job_data(df)\n",
    "print(f'train: {round(train.shape[0]/len(df),2)}')\n",
    "print(f'validate: {round(validate.shape[0]/len(df),2)}')\n",
    "print(f'test: {round(test.shape[0]/len(df),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>is_remote</th>\n",
       "      <th>clean</th>\n",
       "      <th>words</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>DE</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Stefanini, Inc</td>\n",
       "      <td>Richmond, VA</td>\n",
       "      <td>0</td>\n",
       "      <td>stefanini looking data engineer richmond va re...</td>\n",
       "      <td>[stefanini, looking, data, engineer, richmond,...</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>DA</td>\n",
       "      <td>Financial Data Analyst (vehicle retail Domain)</td>\n",
       "      <td>SILVERLINK TECHNOLOGIES LLC</td>\n",
       "      <td>Bowling Green, KY</td>\n",
       "      <td>0</td>\n",
       "      <td>hiplease go let know interestjob title financi...</td>\n",
       "      <td>[hiplease, go, let, know, interestjob, title, ...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>DE</td>\n",
       "      <td>Data Engineer I or II - Can Be Remote Based On...</td>\n",
       "      <td>Associated Bank</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "      <td>1</td>\n",
       "      <td>associated bank equal opportunity employer com...</td>\n",
       "      <td>[associated, bank, equal, opportunity, employe...</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>DS</td>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Gap Inc.</td>\n",
       "      <td>United States</td>\n",
       "      <td>0</td>\n",
       "      <td>gap inc brand bridge gap see world old navy de...</td>\n",
       "      <td>[gap, inc, brand, bridge, gap, see, world, old...</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>MLE</td>\n",
       "      <td>Automation/Artificial Intelligence Machine Lea...</td>\n",
       "      <td>LOCKHEED MARTIN CORPORATION</td>\n",
       "      <td>Littleton, CO 80125</td>\n",
       "      <td>0</td>\n",
       "      <td>coolest job planet lockheed martin space lockh...</td>\n",
       "      <td>[coolest, job, planet, lockheed, martin, space...</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                          job_title  \\\n",
       "420    DE                                      Data Engineer   \n",
       "250    DA     Financial Data Analyst (vehicle retail Domain)   \n",
       "426    DE  Data Engineer I or II - Can Be Remote Based On...   \n",
       "170    DS                           Associate Data Scientist   \n",
       "581   MLE  Automation/Artificial Intelligence Machine Lea...   \n",
       "\n",
       "                         company             location  is_remote  \\\n",
       "420               Stefanini, Inc         Richmond, VA          0   \n",
       "250  SILVERLINK TECHNOLOGIES LLC    Bowling Green, KY          0   \n",
       "426              Associated Bank        Milwaukee, WI          1   \n",
       "170                     Gap Inc.        United States          0   \n",
       "581  LOCKHEED MARTIN CORPORATION  Littleton, CO 80125          0   \n",
       "\n",
       "                                                 clean  \\\n",
       "420  stefanini looking data engineer richmond va re...   \n",
       "250  hiplease go let know interestjob title financi...   \n",
       "426  associated bank equal opportunity employer com...   \n",
       "170  gap inc brand bridge gap see world old navy de...   \n",
       "581  coolest job planet lockheed martin space lockh...   \n",
       "\n",
       "                                                 words  doc_length  \n",
       "420  [stefanini, looking, data, engineer, richmond,...         216  \n",
       "250  [hiplease, go, let, know, interestjob, title, ...          64  \n",
       "426  [associated, bank, equal, opportunity, employe...         366  \n",
       "170  [gap, inc, brand, bridge, gap, see, world, old...         337  \n",
       "581  [coolest, job, planet, lockheed, martin, space...         499  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
